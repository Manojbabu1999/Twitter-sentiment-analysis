# -*- coding: utf-8 -*-
"""EE_695_Project_Sentiment_Analysis_Using_Twitter_Data_2 (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sxqTJid1Kwekb7pqfOtgzKz2nubHdY8A
"""

import pandas as pd
import re
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from nltk import FreqDist, bigrams
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
import nltk
nltk.download('punkt')
nltk.download('stopwords')

# Load your dataset
df = pd.read_csv('/content/Twitter_Data.csv')

# Preview the dataset
print(df.head())

df = df.iloc[:60000,:]

def preprocess_tweet(tweet):
    tweet = str(tweet)
    # Remove URLs
    tweet = re.sub(r"http\S+|www\S+|https\S+", '', tweet, flags=re.MULTILINE)
    # Remove user @ references and '#' from tweet
    tweet = re.sub(r'\@\w+|\#','', tweet)
    # Remove punctuations
    tweet = re.sub(r'[^\w\s]', '', tweet)
    # Make words lowercase
    tweet = tweet.lower()
    # Tokenize the tweet
    tweet_tokens = word_tokenize(tweet)
    # Remove stopwords
    filtered_words = [w for w in tweet_tokens if not w in stopwords.words('english')]

    ps = PorterStemmer()
    stemmed_words = [ps.stem(w) for w in filtered_words]

    return " ".join(stemmed_words)

# Apply the preprocessing function to your dataset
df['processed_tweet'] = df['clean_text'].apply(preprocess_tweet)

print(df['processed_tweet'].head())

print(df.head())
print(df.info())
print(df.isnull().sum())

df = df.dropna(subset=['clean_text'])

# Extract hashtags and mentions
hashtags = df['processed_tweet'].apply(lambda x: [word for word in x.split() if word.startswith('#')])
mentions = df['processed_tweet'].apply(lambda x: [word for word in x.split() if word.startswith('@')])

# Plot the distribution of the number of hashtags and mentions by sentiment
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
sns.countplot(x=hashtags.apply(len), hue=df['category'])
plt.title('Distribution of Hashtags by Sentiment')
plt.xlabel('Number of Hashtags')
plt.ylabel('Count')

plt.subplot(1, 2, 2)
sns.countplot(x=mentions.apply(len), hue=df['category'])
plt.title('Distribution of Mentions by Sentiment')
plt.xlabel('Number of Mentions')
plt.ylabel('Count')

plt.show()

sns.countplot(x='category', data=df)
plt.title('Sentiment Distribution')
plt.show()

df['text_length'] = df['clean_text'].str.len()
sns.histplot(df['text_length'], bins=30)
plt.title('Distribution of Tweet Lengths')
plt.show()

from nltk.tokenize import word_tokenize

all_words = ' '.join(df['processed_tweet']).split()
freq_dist = nltk.FreqDist(all_words)
print(freq_dist.most_common(20))

pd.Series(freq_dist).sort_values(ascending=False).head(20).plot(kind='bar')
plt.title('Top 20 Most Frequent Words')
plt.show()

for category in df['category'].unique():
    category_words = ' '.join(df[df['category'] == category]['processed_tweet'])
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(category_words)
    plt.figure()
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f'Word Cloud for Sentiment Category {category}')
    plt.show()

"""### Implement models"""

#drop the clean_text as text was cleaned and processed into processed_tweet

df.drop(['clean_text'],axis=1,inplace=True)

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.pipeline import Pipeline


#split the data into train and test
X = df['processed_tweet'].values
y = df['category'].values
X_train,X_test,train_labels,test_labels = train_test_split(X, y, test_size = 0.2,random_state=42)

# Define a TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer()

# Define classifiers
classifiers = {
    'Naive Bayes': MultinomialNB(),
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'SVM': SVC(),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42)
}

# Create pipelines for each classifier
pipelines = {}

for name, classifier in classifiers.items():
    pipelines[name] = Pipeline([
        ('tfidf', tfidf_vectorizer),
        ('classifier', classifier)
    ])

# Train and evaluate each classifier using the pipeline
for name, pipeline in pipelines.items():
    print(f"Training {name}...")
    pipeline.fit(X_train,train_labels)

    # Make predictions on the test set
    predictions = pipeline.predict(X_test)

    # Evaluate the model
    accuracy = accuracy_score(test_labels, predictions)
    print(f'Accuracy for {name}: {accuracy:.2f}')

    # Display classification report and confusion matrix
    print(f'\nClassification Report for {name}:')
    print(classification_report(test_labels, predictions))

    print(f'\nConfusion Matrix for {name}:')
    conf_matrix = confusion_matrix(test_labels, predictions)
    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative', 'Neutral', 'Positive'], yticklabels=['Negative', 'Neutral', 'Positive'])
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title(f'Confusion Matrix for {name}')
    plt.show()

"""  ### Implement the LSTM Model"""

from sklearn.preprocessing import LabelEncoder
X = df['processed_tweet']
y = df['category']

#convert labels into postive integers which can be useful while getting max class from predicted probabilities
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(y)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=5000)
tokenizer.fit_on_texts(X)

sequences = tokenizer.texts_to_sequences(X)
padded = pad_sequences(sequences, maxlen=df['text_length'].max())  # maxlen chosen from df

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam

vocab_size = len(tokenizer.word_index) + 1  # Plus 1 for padding
max_length = df['text_length'].max()  # Or the maxlen used in pad_sequences

model = Sequential()
model.add(Embedding(vocab_size, 100, input_length=max_length))  # 100 is the embedding dimension
model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(3, activation='softmax'))

model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])

from sklearn.model_selection import train_test_split

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(padded, y, test_size=0.2, random_state=42)

X_train.shape,y_train.shape

import numpy as np
np.unique(y_train)

# Train the model
early_stopping = EarlyStopping(monitor='val_loss', patience=3)``
history = model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test), callbacks=[early_stopping],batch_size = 64)

from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred,axis=1)  # Convert probabilities to class labels
print(classification_report(y_test, y_pred_classes))

# inverse transform the test and pred labels
y_test_labels = label_encoder.inverse_transform(y_test)
y_pred_labels = label_encoder.inverse_transform(y_pred_classes)

# Create a confusion matrix with the original labels
conf_matrix = confusion_matrix(y_test_labels, y_pred_labels)

# Set up a custom axis ticks and labels
class_labels = [-1, 0, 1]
tick_labels = [str(label) for label in class_labels]

# Plot the confusion matrix
sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', xticklabels=tick_labels, yticklabels=tick_labels)
plt.xlabel('Predicted labels')
plt.ylabel('True labels')
plt.title('Confusion Matrix')
plt.show()

!pip install keras-tuner

from kerastuner.tuners import RandomSearch
import tensorflow as tf

def build_model(hp):
    model = Sequential()
    model.add(Embedding(vocab_size, hp.Int('embedding_dim', min_value=32, max_value=128, step=32),
                        input_length=max_length))
    model.add(LSTM(hp.Int('lstm_units', min_value=32, max_value=128, step=32), dropout=0.2, recurrent_dropout=0.2))
    model.add(Dense(3, activation='softmax'))
    model.compile(optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),
                  loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

tuner = RandomSearch(build_model, objective='val_accuracy', max_trials=2, executions_per_trial=3, directory='my_dir', project_name='hparam_tuning')

tuner.search(X_train, y_train, epochs=5, validation_data=(X_test, y_test))

# Function to predict sentiment
def predict_sentiment(statement):
    sequence = tokenizer.texts_to_sequences([statement])
    padded_sequence = pad_sequences(sequence, maxlen=200)  # Adjust 'maxlen' as per your training setup
    prediction = model.predict(padded_sequence)

    # Assuming your classes are encoded as 0, 1, 2
    class_mapping = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}
    predicted_class = int(prediction.argmax(axis=1))

    sentiment = class_mapping.get(predicted_class, 'Unknown')
    return sentiment

# Example usage
statement = "People hate government"
sentiment = predict_sentiment(statement)
print("Predicted Sentiment:", sentiment)

# Example usage
statement = "People love government"
sentiment = predict_sentiment(statement)
print("Predicted Sentiment:", sentiment)